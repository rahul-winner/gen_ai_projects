{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_groq import ChatGroq\n",
    "## from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# 1. Load your 10 PDFs\n",
    "pdf_folder = \"../my_pdfs\"\n",
    "all_docs = []\n",
    "for file in os.listdir(pdf_folder):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_folder, file))\n",
    "        all_docs.extend(loader.load())\n",
    "\n",
    "all_docs_length = len(all_docs)\n",
    "print(f\"==> Loaded {all_docs_length} documents from PDFs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=25)\n",
    "chunks = splitter.split_documents(all_docs)\n",
    "chunks_length = len(chunks)\n",
    "print(f\"Split into {chunks_length} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create Vector Store\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"../chroma_db\")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"what is the sales of hobbieskart?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a640aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"tell me which request header to set for safeguarding CORS issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0951850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define our State\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    generation: str "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdcbeb",
   "metadata": {},
   "source": [
    "## Graph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b58ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieval Node\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    print(\"---RETRIEVING---\")\n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    print(f\"Retrieved documents : {docs}\")\n",
    "    return {\"documents\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb376249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grading documents Node\n",
    "\n",
    "def grade_documents(state: AgentState):\n",
    "    print(\"---GRADING DOCUMENTS---\")\n",
    "    # Logic: Use an LLM to check if docs match the question\n",
    "    # If yes, return \"generate\"; if no, return \"end\"\n",
    "    return \"generate\" if len(state[\"documents\"]) > 0 else \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41004c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize the Cloud LLM\n",
    "# We set the base_url to the cloud endpoint and provide the API key in headers\n",
    "llm = ChatOllama(\n",
    "    model=\"gpt-oss:120b\", # Or any other cloud-supported model\n",
    "    base_url=\"https://ollama.com\",\n",
    "    headers={\"Authorization\": f\"Bearer {os.environ['OLLAMA_API_KEY']}\"},\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Update the Generate Node\n",
    "\n",
    "def generate(state: AgentState):\n",
    "    print(\"---GENERATING WITH OLLAMA CLOUD---\")\n",
    "    question = state[\"question\"]\n",
    "    # We combine the text from our 10 PDF chunks\n",
    "    context = \"\\n\".join([d.page_content for d in state[\"documents\"]])\n",
    "    print(f\"===>>> Context length: {len(context)} characters.\")\n",
    "    prompt = f\"\"\"You are a strict PDF assistant. \n",
    "    Answer the question using ONLY the context provided.\n",
    "    Context: {context}\n",
    "    Question: {question}\"\"\"\n",
    "    \n",
    "    # The agent invokes the cloud model\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"generation\": response.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734557d2",
   "metadata": {},
   "source": [
    "## BUILDING THE GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e09fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# Add a conditional edge to enforce the \"Strict\" rule\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"end\": END # The agent stops if documents aren't found\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09409dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Replace 'app' with the name of your compiled graph\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f354736",
   "metadata": {},
   "source": [
    "## Test the app / Call the graph / Ask the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare your initial state\n",
    "# inputs = {\"question\": \"Tell me 5 major concepts of Crew AI?\"}\n",
    "# inputs = {\"question\": \"is there any information about hobbies kart? any information on the sale.\"}\n",
    "\n",
    "# 2. Run the graph until it reaches the END node\n",
    "# final_state = app.invoke(inputs)\n",
    "\n",
    "# 3. Access your answer\n",
    "# print(final_state[\"generation\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347cf137",
   "metadata": {},
   "source": [
    "## Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3888f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Wrap your LangGraph app in a simple function\n",
    "def chat_with_agent(message, history):\n",
    "    # Call your graph here\n",
    "    response = app.invoke({\"question\": message})\n",
    "    return response[\"generation\"]\n",
    "\n",
    "# Create and launch the UI\n",
    "gr.ChatInterface(chat_with_agent).launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f94bc1",
   "metadata": {},
   "source": [
    "## Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"ðŸ“„ My PDF Agentic RAG\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"Ask a question about your 10 PDFs\"):\n",
    "    # 1. Show user message\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # 2. Call the LangGraph Agent\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            # We call the 'app' we built earlier\n",
    "            response = app.invoke({\"question\": prompt})\n",
    "            answer = response[\"generation\"]\n",
    "            st.markdown(answer)\n",
    "    \n",
    "    # 3. Save assistant response to history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1716829",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run streamlit run app.py --server.headless true --server.showEmailPrompt false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f814814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
