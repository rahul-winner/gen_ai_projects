{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c994fd8",
   "metadata": {},
   "source": [
    "## Key Technical Insights\n",
    "\n",
    "    ### State Management: \n",
    "\n",
    "    Notice the Annotated[list, operator.add]. This is crucial in LangGraph; it tells the graph to treat the message list as an append-only log, preserving the \"thinking\" history.\n",
    "\n",
    "    ### Tool Binding: \n",
    "    By using .bind_tools(), we provide the LLM with the JSON schema of our Python function. The model doesn't just \"guess\"; it follows the schema.\n",
    "\n",
    "    ### Local Privacy: \n",
    "    Because this uses Ollama, no data leaves your machine. This is the gold standard for enterprise R&D where IP protection is paramount.\n",
    "\n",
    "    ### Intermediate Outputs: \n",
    "    The app.stream method allows Gradio to update the UI as the agent moves through different nodes (e.g., from the LLM node to the Tool node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, TypedDict, Union, List\n",
    "\n",
    "import gradio as gr\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Tools\n",
    "@tool\n",
    "def get_weather(city: str):\n",
    "    \"\"\"Get the current weather for a specific city.\"\"\"\n",
    "    # Logic to fetch weather (Simulated for this expert demo)\n",
    "    if \"san francisco\" in city.lower():\n",
    "        return \"It's 62¬∞F and foggy.\"\n",
    "    return f\"The weather in {city} is 75¬∞F and sunny.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate_distance(source: str, destination: str):\n",
    "    \"\"\"CRITICAL: Use this FIRST to determine travel time and distance for any trip planning.\"\"\"\n",
    "    # Simulated logic for distance calculation\n",
    "    if source.lower() == \"new york\" and destination.lower() == \"los angeles\":\n",
    "        return \"The distance between New York and Los Angeles is approximately 2,450 miles.\"\n",
    "    return f\"The distance between {source} and {destination} is approximately 100 miles.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411734be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Tools (Web Search)\n",
    "from duckduckgo_search import DDGS\n",
    "@tool\n",
    "def web_search(query: str):\n",
    "    \"\"\"Searches the live web for current information.\"\"\"\n",
    "    with DDGS() as ddgs:\n",
    "        results = [r for r in ddgs.text(query, max_results=3)]\n",
    "        return \"\\n\".join([f\"[{r['title']}] {r['body']}\" for r in results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcfffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def travel_advice(destination: str):\n",
    "    \"\"\"ONLY use this for specific sightseeing 'Must-Do' lists for Mumbai, Pune, Kolkata, etc.\"\"\"\n",
    "    advice = {\n",
    "        \"udaipur\": \"Don't miss the City Palace and take a boat ride on Lake Pichola!\",\n",
    "        \"kolkata\": \"Visit the Victoria Memorial and try the local street food.\",\n",
    "        \"mumbai\": \"Explore the Gateway of India and enjoy a walk along Marine Drive.\",\n",
    "    }\n",
    "    return advice.get(destination.lower(), f\"Sorry, I don't have travel advice for {destination}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather, calculate_distance, travel_advice, web_search]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup the Model with Tools\n",
    "# We use Ollama locally. 'llama3.2' supports tool calling natively.\n",
    "model = ChatOllama(model=\"llama3.2\", temperature=0).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a640aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Graph State\n",
    "class AgentState(TypedDict):\n",
    "    # The 'operator.add' allows us to append messages rather than overwrite\n",
    "    messages: Annotated[list[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0951850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Logic Nodes\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert Travel Planner. To provide a high-quality response, you MUST follow these steps:\\n\"\n",
    "    \"1. BREAK DOWN the user request into sub-tasks (Distance, Weather, Local Tips).\\n\"\n",
    "    \"2. EXECUTE tools for each sub-task. Do NOT provide a final answer until you have checked distance AND weather AND travel advice.\\n\"\n",
    "    \"3. If a tool doesn't have the specific city (like Mumbai), use 'web_search' to find the info.\\n\"\n",
    "    \"4. Only provide the final itinerary AFTER all tool results are gathered.\"\n",
    ")\n",
    "\n",
    "\n",
    "def call_model(state: AgentState):\n",
    "    # Fix: Correctly pass the list containing the system prompt\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Check if system prompt exists, if not, prepend it\n",
    "    if not any(isinstance(m, SystemMessage) for m in messages):\n",
    "        messages = [SystemMessage(content=system_prompt)] + messages\n",
    "\n",
    "    # IMPORTANT: Use 'messages' variable, not 'state[\"messages\"]'\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b58ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Router to determine the next step.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb376249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Build the Graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09409dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Replace 'app' with the name of your compiled graph\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41004c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ENHANCED UI Logic for Multi-Turn Reasoning ---\n",
    "def agent_chat(user_input, history):\n",
    "    # Start with a fresh state for each new query\n",
    "    inputs = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "    \n",
    "    full_display_content = \"\"\n",
    "    step_num = 1\n",
    "    \n",
    "    # We use stream to capture every transition in the graph\n",
    "    for output in app.stream(inputs, stream_mode=\"values\"):\n",
    "        last_msg = output[\"messages\"][-1]\n",
    "        \n",
    "        # 1. AGENT IS THINKING/PLANNING\n",
    "        if isinstance(last_msg, AIMessage) and last_msg.tool_calls:\n",
    "            for tool_call in last_msg.tool_calls:\n",
    "                full_display_content += f\"ü§î **Step {step_num}: Thinking...**\\n\"\n",
    "                full_display_content += f\"   - Decision: I need to use `{tool_call['name']}`\\n\"\n",
    "                full_display_content += f\"   - Input: `{tool_call['args']}`\\n\\n\"\n",
    "            yield full_display_content\n",
    "            \n",
    "        # 2. TOOL IS EXECUTING\n",
    "        elif isinstance(last_msg, ToolMessage):\n",
    "            full_display_content += f\"üì• **Step {step_num}: Observation**\\n\"\n",
    "            full_display_content += f\"   - Result: {last_msg.content[:200]}...\\n\\n\"\n",
    "            step_num += 1 # Increment only after tool returns\n",
    "            yield full_display_content\n",
    "            \n",
    "        # 3. FINAL SYNTHESIS\n",
    "        elif isinstance(last_msg, AIMessage) and not last_msg.tool_calls:\n",
    "            full_display_content += \"--- \\n### üèÅ Final Answer\\n\"\n",
    "            full_display_content += last_msg.content\n",
    "            yield full_display_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac387d1",
   "metadata": {},
   "source": [
    "## Sample questions\n",
    "- I live in Dhanori, Pune. I am planning a trip to Shaniwar wada Pune. Please plan an itinerary for this short trip. mention the distances and transport choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814b4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch UI\n",
    "view = gr.ChatInterface(\n",
    "    fn=agent_chat,\n",
    "    title=\"LangGraph Local Agent (Ollama)\",\n",
    "    description=\"Ask me about travel, climate etc. . I'll show you my thinking process.\",\n",
    "    examples=[\"What's the weather in San Francisco?\", \"Tell me a joke.\"]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1088164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
