{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c994fd8",
      "metadata": {
        "id": "7c994fd8"
      },
      "source": [
        "## Key Technical Insights\n",
        "\n",
        "### State Management:\n",
        "Notice the Annotated[list, operator.add]. This is crucial in LangGraph; it tells the graph to treat the message list as an append-only log, preserving the \"thinking\" history.\n",
        "\n",
        "### Tool Binding:\n",
        "By using .bind_tools(), we provide the LLM with the JSON schema of our Python function. The model doesn't just \"guess\"; it follows the schema.\n",
        "\n",
        "### Local Privacy:\n",
        "Because this uses Ollama, no data leaves your machine. This is the gold standard for enterprise R&D where IP protection is paramount.\n",
        "\n",
        "### Intermediate Outputs:\n",
        "The app.stream method allows Gradio to update the UI as the agent moves through different nodes (e.g., from the LLM node to the Tool node)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab setup"
      ],
      "metadata": {
        "id": "I316unqrUgVc"
      },
      "id": "I316unqrUgVc"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install uv\n",
        "!pip install -q uv\n",
        "\n",
        "# 2. Download the TOML file to the local Colab environment\n",
        "!wget https://raw.githubusercontent.com/rahul-winner/gen_ai_projects/refs/heads/main/pyproject.toml -O pyproject.toml\n",
        "\n",
        "# 3. Install the dependencies specifically from the pyproject.toml file\n",
        "!uv pip install --system -r pyproject.toml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UHVKixwVmRW",
        "outputId": "4b7ea474-0b31-4897-e29b-7acee364881f"
      },
      "id": "8UHVKixwVmRW",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-26 06:32:38--  https://raw.githubusercontent.com/rahul-winner/gen_ai_projects/refs/heads/main/pyproject.toml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 569 [text/plain]\n",
            "Saving to: ‚Äòpyproject.toml‚Äô\n",
            "\n",
            "pyproject.toml      100%[===================>]     569  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-26 06:32:38 (16.8 MB/s) - ‚Äòpyproject.toml‚Äô saved [569/569]\n",
            "\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m171 packages\u001b[0m \u001b[2min 2.86s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m46 packages\u001b[0m \u001b[2min 3.71s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m7 packages\u001b[0m \u001b[2min 340ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m46 packages\u001b[0m \u001b[2min 158ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbcrypt\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbuild\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mchromadb\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdataclasses-json\u001b[0m\u001b[2m==0.6.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mddgs\u001b[0m\u001b[2m==9.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdurationpy\u001b[0m\u001b[2m==0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfake-useragent\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.45.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.55.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.56.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==5.50.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==6.2.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgroq\u001b[0m\u001b[2m==0.37.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mkubernetes\u001b[0m\u001b[2m==34.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-chroma\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.2.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-google-genai\u001b[0m\u001b[2m==4.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-groq\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-ollama\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.1.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmypy-extensions\u001b[0m\u001b[2m==1.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.37.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mposthog\u001b[0m\u001b[2m==5.4.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprimp\u001b[0m\u001b[2m==0.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpybase64\u001b[0m\u001b[2m==1.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydeck\u001b[0m\u001b[2m==0.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypdf\u001b[0m\u001b[2m==6.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpypika\u001b[0m\u001b[2m==0.48.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyproject-hooks\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msocksio\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstreamlit\u001b[0m\u001b[2m==1.52.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspect\u001b[0m\u001b[2m==0.9.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.1\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting for .env file"
      ],
      "metadata": {
        "id": "JOPGjcXIX-Qv"
      },
      "id": "JOPGjcXIX-Qv"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Install python-dotenv\n",
        "!pip install python-dotenv -q\n",
        "\n",
        "# 3. Load the file from your Drive path\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "env_path = '/content/drive/MyDrive/ai_api_keys/.env'\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# 4. Verify\n",
        "print(os.getenv(\"LANGCHAIN_PROJECT\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1Sy2SYZXueZ",
        "outputId": "3e554e5a-ebb0-403a-d00a-8fa5de7d2f16"
      },
      "id": "v1Sy2SYZXueZ",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "pr-giving-strait-89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actual code starts from here"
      ],
      "metadata": {
        "id": "XVqQkqNKV4LM"
      },
      "id": "XVqQkqNKV4LM"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "46c2317b",
      "metadata": {
        "id": "46c2317b"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated, TypedDict\n",
        "\n",
        "import gradio as gr\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
        "from langchain_core.tools import tool, StructuredTool\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24bd534f",
      "metadata": {
        "id": "24bd534f"
      },
      "source": [
        "### Langsmith integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "28550d3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28550d3f",
        "outputId": "ddc26ded-a5ff-48fe-b95e-835dfdbd08c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing enabled: true\n",
            "Project: pr-giving-strait-89\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv(dotenv_path='../.env')\n",
        "\n",
        "# Verify tracing is enabled\n",
        "print(f\"Tracing enabled: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
        "print(f\"Project: {os.getenv('LANGCHAIN_PROJECT')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "411734be",
      "metadata": {
        "id": "411734be"
      },
      "outputs": [],
      "source": [
        "def company_charges_fn(source: str, destination: str, days: int) -> float:\n",
        "    \"\"\"Looks up current service charges for a given company.\"\"\"\n",
        "    return 50.0 + (days * 10.0)  # Dummy implementation\n",
        "\n",
        "company_service_charges = StructuredTool.from_function(\n",
        "    func=company_charges_fn,\n",
        "    name=\"company_service_charges\",\n",
        "    description=\"Useful for calculating service charges based on source, destination, and number of days.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ce4d9b57",
      "metadata": {
        "id": "ce4d9b57"
      },
      "outputs": [],
      "source": [
        "def company_affiliations_fn(source: str, destination: str) -> str:\n",
        "    \"\"\"Looks up current affiliations for a given company.\"\"\"\n",
        "    return \"\"\"\n",
        "        Recommend following following hotels: 'MSTDC hotels and resorts', 'The Fern', 'Taj Hotels', 'Sid OYO Rooms'.\n",
        "        Recommend following ferries: 'Goa Ferries', 'Mandovi River Cruises'.\n",
        "        Recommend following restaurants: 'MSTDC Restaurants', 'Venite Bar and Restaurant', 'Fisherman's Wharf'.\n",
        "        Recommend following rentals: 'Zoomcar', 'MSTDC Vehicle Rentals', 'Savaari Car Rentals'.\n",
        "    \"\"\"\n",
        "\n",
        "company_affiliations = StructuredTool.from_function(\n",
        "    func=company_affiliations_fn,\n",
        "    name=\"company_affiliations_service\",\n",
        "    description=\"Useful for looking up company affiliations to suggest hotels and other services like ferry, restaurant, rentals etc.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3dcfffde",
      "metadata": {
        "id": "3dcfffde"
      },
      "outputs": [],
      "source": [
        "def book_flight_func(source: str, destination: str) -> str:\n",
        "    \"\"\"\n",
        "    Internal implementation: Books a flight ticket.\n",
        "\n",
        "    Args:\n",
        "        source: Departure city\n",
        "        destination: Arrival city\n",
        "\n",
        "    Returns:\n",
        "        Confirmation message\n",
        "    \"\"\"\n",
        "    return f\"Flight booked from {source} to {destination} with Airline XYZ. Confirmation number: ABC123.\"\n",
        "\n",
        "book_flight = StructuredTool.from_function(\n",
        "    func=book_flight_func,\n",
        "    name=\"flight_booking_service\",\n",
        "    description=\"Books flights between major cities. Requires source and destination city names.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3a3006b0",
      "metadata": {
        "id": "3a3006b0"
      },
      "outputs": [],
      "source": [
        "def book_hotel_fn(city: str, days: int) -> str:\n",
        "    \"\"\"Use this for booking hotels/resorts in major cities.\"\"\"\n",
        "    return f\"Hotel booked in {city}, for {days} days in Marriot hotel.\"\n",
        "\n",
        "book_hotel = StructuredTool.from_function(\n",
        "    func=book_hotel_fn,\n",
        "    name=\"hotel_booking_service\",\n",
        "    description=\"Books hotels in major cities. Requires city name and number of days.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "68a640aa",
      "metadata": {
        "id": "68a640aa"
      },
      "outputs": [],
      "source": [
        "# 3. Define Graph State\n",
        "class AgentState(TypedDict):\n",
        "    # The 'operator.add' allows us to append messages rather than overwrite\n",
        "    messages: Annotated[list[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "818e6dd8",
      "metadata": {
        "id": "818e6dd8"
      },
      "outputs": [],
      "source": [
        "tools = [company_service_charges, company_affiliations, book_hotel, book_flight]\n",
        "tool_node = ToolNode(tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea6a919",
      "metadata": {
        "id": "7ea6a919"
      },
      "source": [
        "## Planner node\n",
        "\n",
        "Note - I observed that planner node creates lot of confusion, specially when we bind tools with the model. Sometiems if fails to call the tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4968bd81",
      "metadata": {
        "id": "4968bd81"
      },
      "outputs": [],
      "source": [
        "# 2. Setup the Model with Tools\n",
        "from pydantic import SecretStr\n",
        "\n",
        "planner_model = ChatOpenAI(\n",
        "    model=\"gpt-oss:120b\",\n",
        "    base_url=\"https://ollama.com/v1\",\n",
        "    api_key=SecretStr(os.environ['OLLAMA_API_KEY']),\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f7aff23c",
      "metadata": {
        "id": "f7aff23c"
      },
      "outputs": [],
      "source": [
        "def create_plan(state: AgentState):\n",
        "    \"\"\"Creates an execution plan with tool awareness.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Get the user's question\n",
        "    user_message = next((m for m in reversed(messages) if isinstance(m, HumanMessage)), None)\n",
        "\n",
        "    if user_message:\n",
        "        # List available tools in the prompt\n",
        "        tool_descriptions = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
        "\n",
        "        enhanced_planner_prompt = (\n",
        "            \"You are an AI Agent Planning Expert. Analyze the user's request and create a step-by-step execution plan.\\n\"\n",
        "            \"Break down the request into specific sub-tasks that need to be completed.\\n\\n\"\n",
        "            f\"Available tools:\\n{tool_descriptions}\\n\\n\"\n",
        "            \"For each step, identify:\\n\"\n",
        "            \"1. What needs to be done\\n\"\n",
        "            \"2. Which tool should be used (if applicable)\\n\"\n",
        "            \"3. What information to gather or action to perform\\n\\n\"\n",
        "            \"Create ONLY the plan in text format. Do NOT execute any tools.\"\n",
        "        )\n",
        "\n",
        "        planning_messages = [\n",
        "            SystemMessage(content=enhanced_planner_prompt),\n",
        "            HumanMessage(content=f\"User request: {user_message.content}\")\n",
        "        ]\n",
        "\n",
        "        # Get plan\n",
        "        response = planner_model.invoke(planning_messages)\n",
        "\n",
        "        # Handle response content\n",
        "        plan = response.content if isinstance(response.content, str) else str(response.content)\n",
        "\n",
        "        if plan and plan.strip():\n",
        "            plan_message = AIMessage(content=f\"üìã **Execution Plan:**\\n\\n{plan}\\n\\n---\\n\")\n",
        "            return {\"messages\": [plan_message]}\n",
        "        else:\n",
        "            # Fallback - skip planning if it fails\n",
        "            print(\"WARNING: Plan generation failed, skipping to agent\")\n",
        "            return {\"messages\": []}\n",
        "\n",
        "    return {\"messages\": []}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f6d1bccf",
      "metadata": {
        "id": "f6d1bccf"
      },
      "outputs": [],
      "source": [
        "# 2. Setup the Model with Tools\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-oss:120b\",\n",
        "    base_url=\"https://ollama.com/v1\",\n",
        "    api_key=SecretStr(os.environ['OLLAMA_API_KEY']),\n",
        "    temperature=0\n",
        ").bind_tools(tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d0951850",
      "metadata": {
        "id": "d0951850"
      },
      "outputs": [],
      "source": [
        "# 4. Define Logic Nodes\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an expert Travel Planner. To provide a high-quality response, you MUST follow these steps:\\n\"\n",
        "    \"1. MUST call and give precedence to provided tools.\\n\"\n",
        "    \"4. Use 'flight_booking_service' to book flights and 'hotel_booking_service' to book accommodations.\\n\"\n",
        "    \"5. Only provide the final itinerary AFTER all tool results are gathered.\\n\"\n",
        "    \"6. Recommend hotels, ferries, restaurants, rentals based on company affiliations using 'company_affiliations_service' if required.\\n\"\n",
        "    \"7. Make some reasonable assumptions about dates and preferences if not provided by the user. NO NEED TO ASK FOR CLARIFICATION.\\n\"\n",
        "    \"** IMPORTANT **\\n\"\n",
        "    \"- Do not show obscenities in the final response.\\n\"\n",
        "    \"Note: Execute the tools as per the plan step-by-step, ensuring each tool's output is considered before finalizing your response.\"\n",
        ")\n",
        "\n",
        "\n",
        "def call_model(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Find if there's a plan message\n",
        "    print(f\"==>>> DEBUG: Checking for plan message in messages: {[str(m.content) for m in messages]}\")\n",
        "    plan_message = next((m for m in messages if isinstance(m, AIMessage) and \"üìã **Execution Plan:**\" in str(m.content)), None)\n",
        "\n",
        "    # Build enhanced system prompt with plan if available\n",
        "    enhanced_system_prompt = system_prompt\n",
        "    if plan_message:\n",
        "        plan_text = plan_message.content.replace(\"üìã **Execution Plan:**\", \"\").replace(\"---\", \"\").strip()\n",
        "        enhanced_system_prompt = (\n",
        "            f\"{system_prompt}\\n\\n\"\n",
        "            f\"EXECUTION PLAN TO FOLLOW:\\n{plan_text}\\n\\n\"\n",
        "            f\"Execute this plan step by step using the available tools.\"\n",
        "        )\n",
        "        # Remove plan message from conversation\n",
        "        messages = [m for m in messages if m != plan_message]\n",
        "\n",
        "    # Add/update system prompt\n",
        "    if not any(isinstance(m, SystemMessage) for m in messages):\n",
        "        messages = [SystemMessage(content=enhanced_system_prompt)] + messages\n",
        "    else:\n",
        "        # Replace existing system message\n",
        "        messages = [SystemMessage(content=enhanced_system_prompt)] + [m for m in messages if not isinstance(m, SystemMessage)]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": [response]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "83b58ec9",
      "metadata": {
        "id": "83b58ec9"
      },
      "outputs": [],
      "source": [
        "def should_continue(state: AgentState):\n",
        "    \"\"\"Router to determine the next step.\"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a156de8",
      "metadata": {
        "id": "8a156de8"
      },
      "source": [
        "## Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "eb376249",
      "metadata": {
        "id": "eb376249"
      },
      "outputs": [],
      "source": [
        "# 5. Build the Graph - UPDATED with planner node\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"planner\", create_plan)  # Planning node with tool awareness\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Set flow\n",
        "workflow.set_entry_point(\"planner\")  # Start with planning\n",
        "workflow.add_edge(\"planner\", \"agent\")  # Plan -> Execute\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, {\"tools\": \"tools\", END: END})\n",
        "workflow.add_edge(\"tools\", \"agent\")  # After tools, go back to agent\n",
        "\n",
        "app = workflow.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "41004c82",
      "metadata": {
        "id": "41004c82"
      },
      "outputs": [],
      "source": [
        "# --- ENHANCED UI Logic for Multi-Turn Reasoning ---\n",
        "def agent_chat(user_input, history):\n",
        "    # Start with a fresh state for each new query\n",
        "    inputs = AgentState({\"messages\": [HumanMessage(content=user_input)]})\n",
        "\n",
        "    full_display_content = \"\"  # This accumulates for final display\n",
        "    step_num = 1\n",
        "    processed_message_count = 0  # Track how many messages we've processed\n",
        "\n",
        "    # We use stream to capture every transition in the graph\n",
        "    for output in app.stream(inputs, stream_mode=\"values\"):\n",
        "        messages = output[\"messages\"]\n",
        "\n",
        "        # Process only NEW messages we haven't seen yet\n",
        "        new_messages = messages[processed_message_count:]\n",
        "\n",
        "        for msg in new_messages:\n",
        "            # 1. AGENT IS THINKING/PLANNING\n",
        "            if isinstance(msg, AIMessage) and msg.tool_calls:\n",
        "                for tool_call in msg.tool_calls:\n",
        "                    tool_name = tool_call[\"name\"]\n",
        "                    thinking_message = \"ü§î thinking...\\n\"\n",
        "                    thinking_message += f\"   - checking {tool_name.replace('_', ' ')}...`\\n\\n\"\n",
        "\n",
        "                    # Add to full content for final display\n",
        "                    full_display_content += thinking_message\n",
        "                    step_num += 1\n",
        "\n",
        "                    # Yield ONLY current step (not accumulated content)\n",
        "                    yield thinking_message\n",
        "                    time.sleep(1)  # Simulate thinking time\n",
        "\n",
        "            # 2. TOOL IS EXECUTING\n",
        "            elif isinstance(msg, ToolMessage):\n",
        "                tool_name = msg.name or \"\"\n",
        "                result_message = f\"üì•  {tool_name.replace('_', ' ')} response: \\n\"\n",
        "                result_message += f\"   - Output: {msg.content[:200]}...\\n\\n\"\n",
        "\n",
        "                # Add to full content for final display\n",
        "                full_display_content += result_message\n",
        "\n",
        "                # Yield ONLY current result (not accumulated content)\n",
        "                yield result_message\n",
        "                time.sleep(1)  # Simulate execution time\n",
        "\n",
        "            # 3. FINAL SYNTHESIS - Show complete flow\n",
        "            elif isinstance(msg, AIMessage) and not msg.tool_calls and \"**Execution Plan:**\" not in msg.content:\n",
        "                final_output = full_display_content  # All accumulated steps\n",
        "                final_output += \"---\\n### üèÅ Final Answer\\n\\n\"\n",
        "                content = msg.content if isinstance(msg.content, str) else str(msg.content)\n",
        "                final_output += content\n",
        "\n",
        "                # Yield complete flow with final answer\n",
        "                yield final_output\n",
        "\n",
        "        # Update the count of processed messages\n",
        "        processed_message_count = len(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "09409dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "09409dae",
        "outputId": "d689a5f1-3b01-4850-c651-7db7d2374ab2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Replace 'app' with the name of your compiled graph\n",
        "display(Image(app.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "b814b4fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b814b4fc",
        "outputId": "5acb8d38-3f85-4313-957e-aa2eb57e1a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m65 packages\u001b[0m \u001b[2min 1.03s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m65 packages\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m65 packages\u001b[0m \u001b[2min 158ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m65 packages\u001b[0m \u001b[2min 182ms\u001b[0m\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1maiofiles\u001b[0m\u001b[2m==23.2.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==5.5.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mannotated-doc\u001b[0m\u001b[2m==0.0.4\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.12.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.127.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mffmpy\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.61.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.12.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mgradio\u001b[0m\u001b[2m==4.36.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mgradio-client\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.2.3\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mimportlib-resources\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==2.1.5\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.8\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mnarwhals\u001b[0m\u001b[2m==2.14.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.5\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpydub\u001b[0m\u001b[2m==0.25.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.3.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.21\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.37.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.2.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.30.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mruff\u001b[0m\u001b[2m==0.14.10\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1msemantic-version\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.50.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.12.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.3\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.6.2\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.40.0\u001b[0m\n",
            " \u001b[33m~\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==11.0.3\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-118 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n",
            "Exception in thread Thread-119 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n",
            "Exception in thread Thread-120 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n",
            "Exception in thread Thread-121 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n",
            "Exception in thread Thread-122 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n",
            "Exception in thread Thread-123 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n",
            "Exception in thread Thread-124 (run):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio_run(self.serve(sockets=sockets), loop_factory=self.config.get_loop_factory())\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 71, in serve\n",
            "    await self._serve(sockets)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 78, in _serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/config.py\", line 431, in load\n",
            "    ws_protocol_class = import_from_string(WS_PROTOCOLS.get(self.ws, self.ws))\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 22, in import_from_string\n",
            "    raise exc from None\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/auto.py\", line 19, in <module>\n",
            "    from uvicorn.protocols.websockets.websockets_impl import WebSocketProtocol\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/websockets/websockets_impl.py\", line 16, in <module>\n",
            "    from websockets.legacy.server import HTTPResponse\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/legacy/server.py\", line 29, in <module>\n",
            "    from ..exceptions import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 91, in __getattr__\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/websockets/imports.py\", line 28, in import_name\n",
            "    return getattr(module, name)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'websockets.legacy.exceptions'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2710579480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, mcp_server, _frontend, i18n)\u001b[0m\n\u001b[1;32m   2718\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_endpoints\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2720\u001b[0;31m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m             \u001b[0mdependency_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"parameters\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"returns\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"show_api\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_api\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mstart_server\u001b[0;34m(app, server_name, server_port, ssl_keyfile, ssl_certfile, ssl_keyfile_password)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 )\n\u001b[1;32m    148\u001b[0m             \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_in_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mServerFailedToStartError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\u001b[0m in \u001b[0;36mrun_in_thread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 raise ServerFailedToStartError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Launch UI\n",
        "view = gr.ChatInterface(\n",
        "    fn=agent_chat,\n",
        "    title=\"LangGraph Local Agent (Ollama)\",\n",
        "    description=\"Ask me about travel, climate etc. . I'll show you my thinking process.\",\n",
        "    examples=[\"Create an itinerary for a 5 days trip to Goa from Pune for 1st week of January 2026. Dates are flexible and do the bookings.\",\n",
        "              \"please plan a 3 days trip to bhutan from Pune in first week of February 2026. Include hotel and flight bookings.\"]\n",
        ")\n",
        "\n",
        "view.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gen-ai-projects",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}